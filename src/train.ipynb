{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from cleaner import to_dotless_text, clean_text\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('../dataset/cleaned_train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['dotless'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"word index: {len(word_index)}\")\n",
    "\n",
    "dotless_sequences = tokenizer.texts_to_sequences(data['dotless'])\n",
    "padded_dotless = pad_sequences(dotless_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "print(f\"padded shape: {padded_dotless.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"word index: {len(word_index)}\")\n",
    "\n",
    "text_sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "padded_text = pad_sequences(text_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "print(f\"padded shape: {padded_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = padded_dotless\n",
    "y = padded_text\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train an RNN model with architecture: Embedding 256 -> Bigru 256 -> BIGRU 256 -> Dense 1024 -> Dropout 0.5 -> Dense \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 256, input_length=100),\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    Bidirectional(LSTM(256, return_sequences=True)),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10000, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, 100))  # Assuming input sequence length is 100\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "model.save('rnn_model.h5')\n",
    "\n",
    "# test the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('rnn_model.h5')\n",
    "\n",
    "def predict_text(text):\n",
    "    text = clean_text(text)\n",
    "    dotless = to_dotless_text(text)\n",
    "    sequence = tokenizer.texts_to_sequences([dotless])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=100, padding='post', truncating='post')\n",
    "    prediction = model.predict(padded_sequence)\n",
    "    return prediction\n",
    "\n",
    "text_arabic =\"ويكيبيديا مشروع تعاوني متعدد اللغات يضم ويكيات بأكثر من\"\n",
    "dotless_text_arabic = to_dotless_text(text_arabic)\n",
    "prediction = predict_text(dotless_text_arabic)\n",
    "print(f\"prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
