{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from arabic_reshaper import reshape\n",
    "from bidi.algorithm import get_display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Download NLTK resources if not already installed\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dotless_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>التشكيلات الحضارية المشرقية الفارسية التركية ا...</td>\n",
       "      <td>الٮسكٮلاٮ الحصارٮه المسرڡٮه الڡارسٮه الٮركٮه ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>بومبيو ووزير الدفاع جيمس ماتيس يوم الثلاثا إلى</td>\n",
       "      <td>ٮومٮٮو وورٮر الدڡاع حٮمس ماٮٮس ٮوم الٮلاٮا إلى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>السما والأرض إلا عليهماوفي جلا العيون عن ابي</td>\n",
       "      <td>السما والأرص إلا علٮهماوڡى حلا العٮوں عں اٮى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>إن الحكومة الائتلافية لا تزال مستقرة وستستمر في</td>\n",
       "      <td>إں الحكومه الائٮلاڡٮه لا ٮرال مسٮڡره وسٮسٮمر ڡى</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>تكتمل الاجرائات قبل اجازه العيد قدمت للتقاعد من</td>\n",
       "      <td>ٮكٮمل الاحرائاٮ ڡٮل احاره العٮد ڡدمٮ للٮڡاعد مں</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  التشكيلات الحضارية المشرقية الفارسية التركية ا...   \n",
       "1     بومبيو ووزير الدفاع جيمس ماتيس يوم الثلاثا إلى   \n",
       "2       السما والأرض إلا عليهماوفي جلا العيون عن ابي   \n",
       "3    إن الحكومة الائتلافية لا تزال مستقرة وستستمر في   \n",
       "4    تكتمل الاجرائات قبل اجازه العيد قدمت للتقاعد من   \n",
       "\n",
       "                                        dotless_text  \n",
       "0  الٮسكٮلاٮ الحصارٮه المسرڡٮه الڡارسٮه الٮركٮه ا...  \n",
       "1     ٮومٮٮو وورٮر الدڡاع حٮمس ماٮٮس ٮوم الٮلاٮا إلى  \n",
       "2       السما والأرص إلا علٮهماوڡى حلا العٮوں عں اٮى  \n",
       "3    إں الحكومه الائٮلاڡٮه لا ٮرال مسٮڡره وسٮسٮمر ڡى  \n",
       "4    ٮكٮمل الاحرائاٮ ڡٮل احاره العٮد ڡدمٮ للٮڡاعد مں  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data from csv file\n",
    "data = pd.read_csv('clean_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4446330, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_arabic_text(text, remove_stopwords=False):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Optionally remove stopwords\n",
    "    if remove_stopwords:\n",
    "        arabic_stopwords = set(stopwords.words('arabic'))  \n",
    "        words = [word for word in words if word not in arabic_stopwords]\n",
    "\n",
    "    # Remove non-Arabic characters\n",
    "    filtered_words = [reshape(word) for word in words if any('\\u0600' <= c <= '\\u06FF' for c in word)]\n",
    "\n",
    "\n",
    "    # Join words back into a single string\n",
    "    preprocessed_text = ' '.join(filtered_words)\n",
    "\n",
    "    return preprocessed_text\n",
    "\n",
    "# preprocess text data and dotless text\n",
    "data['text'] = data['text'].apply(lambda x: preprocess_arabic_text(x, remove_stopwords=False))\n",
    "print('text done')\n",
    "data['dotless_text'] = data['dotless_text'].apply(lambda x: preprocess_arabic_text(x, remove_stopwords=False))\n",
    "print('dotless_text done')\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['التشكيلات', 'الحضارية', 'المشرقية', 'الفارسية', 'التركية', 'الهندية', 'المغولية', 'في']\n"
     ]
    }
   ],
   "source": [
    "# Download necessary resources for tokenization\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokenize input text into individual words.\n",
    "    \n",
    "    Args:\n",
    "    - text (str): Input text to tokenize.\n",
    "    \n",
    "    Returns:\n",
    "    - tokens (list): List of tokens (words) extracted from the input text.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# tokenize text data and dotless text\n",
    "data['tokens'] = data['text'].apply(tokenize_text)\n",
    "data['dotless_tokens'] = data['dotless_text'].apply(tokenize_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(texts):\n",
    "    \"\"\"\n",
    "    Create a vocabulary from a list of text sequences.\n",
    "    \n",
    "    Args:\n",
    "    - texts (list): List of text sequences.\n",
    "    \n",
    "    Returns:\n",
    "    - vocab (dict): Vocabulary mapping tokens to unique indices.\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    for text in texts:\n",
    "        for token in text:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "    return vocab\n",
    "\n",
    "# Create vocabulary for text data\n",
    "vocab = create_vocabulary(data['tokens'])\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "\n",
    "# Create vocabulary for dotless text data\n",
    "dotless_vocab = create_vocabulary(data['dotless_tokens'])\n",
    "print(f'Dotless vocabulary size: {len(dotless_vocab)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
